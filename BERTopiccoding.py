import pandas as pd
from bertopic import BERTopic
from sklearn.decomposition import PCA
import numpy as np

# B∆∞·ªõc 1: ƒê·ªçc d·ªØ li·ªáu ƒë√£ c√≥ embedding
df = pd.read_csv("tweets_with_phobert_vectors.csv", encoding="utf-8-sig")

# B∆∞·ªõc 2: L·∫•y danh s√°ch vƒÉn b·∫£n v√† embedding
docs = df['Text_Segmented'].tolist()
embedding_columns = df.columns[4:]  # B·ªè 3 c·ªôt ƒë·∫ßu l√†: Keyword, Text_Clean, Created_At
embeddings = df[embedding_columns].values.tolist()
embeddings = np.array(embeddings)

# (T√πy ch·ªçn) Gi·∫£m chi·ªÅu embedding xu·ªëng 50 chi·ªÅu n·∫øu th·∫•y ch·∫≠m
# from sklearn.decomposition import PCA
# pca = PCA(n_components=50)
# embeddings = pca.fit_transform(embeddings)

# B∆∞·ªõc 3: T·∫°o m√¥ h√¨nh BERTopic (d√πng embedding t·ª´ PhoBERT)
topic_model = BERTopic(min_topic_size=5 ,language="multilingual", verbose=True) #
topics, probs = topic_model.fit_transform(docs, embeddings)
topic_model.save("bertopic_phobert_model")
print("\n‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh BERTopic v√†o th∆∞ m·ª•c: bertopic_phobert_model")


# B∆∞·ªõc 4: In ra 5 ch·ªß ƒë·ªÅ ph·ªï bi·∫øn nh·∫•t
print("\nüìå Top 5 ch·ªß ƒë·ªÅ ƒë∆∞·ª£c ph√°t hi·ªán:")
print(topic_model.get_topic_info().head())
print(topic_model.get_topic(3))
doc_info = topic_model.get_document_info(docs)
doc_info.to_csv("document_topic_info.csv", index=False, encoding='utf-8-sig')
print("\n‚úÖ ƒê√£ l∆∞u chi ti·∫øt vƒÉn b·∫£n v√† ch·ªß ƒë·ªÅ v√†o file: document_topic_info.csv")
# B∆∞·ªõc 5: L∆∞u k·∫øt qu·∫£ ra file CSV m·ªõi
df['Topic'] = topics
df.to_csv("tweets_with_topics.csv", index=False, encoding='utf-8-sig')
print("\n‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ ph√¢n ch·ªß ƒë·ªÅ v√†o file: tweets_with_topics.csv")

# (T√πy ch·ªçn) Tr·ª±c quan h√≥a
#topic_model.visualize_topics().show()

